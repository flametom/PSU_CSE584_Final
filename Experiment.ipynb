{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa1191-137e-4c99-bac0-0ad1a23d5539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y transformers\n",
    "!pip install \"transformers<4.46.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540b83f-2561-4357-a998-d95a413876ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y sentence-transformers\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11893a3-4e9b-4237-ab90-e8249c8bd28b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install openai anthropic google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b8431-125a-40bd-a00f-29f9f07baad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y accelerate\n",
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed27c0-a00d-4484-bd5a-4368a7db03d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98d9f3a3-d9c2-4365-8b43-5e15c7194883",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 19:08:56.621972: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-01 19:08:56.622085: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-01 19:08:56.718787: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-01 19:08:56.919654: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-01 19:08:58.801717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"your_token_of_huggingface\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import openai\n",
    "import anthropic\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "\n",
    "@dataclass\n",
    "class Question:\n",
    "    discipline: str\n",
    "    content: str\n",
    "    fault_reason: str\n",
    "\n",
    "class LLMBase:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.error_detection_instruction = (\n",
    "            \"Provide your solution.\"\n",
    "        )\n",
    "        \n",
    "        self.error_correction_instruction = (\n",
    "            \"Your previous answer to this problem may have contained errors. \"\n",
    "            \"Please review the problem again, identify any potential mistakes, \"\n",
    "            \"and provide a corrected answer if necessary.\"\n",
    "        )\n",
    "        \n",
    "        self.context_consistency_instruction = (\n",
    "            \"Let's analyze this problem one more time. \"\n",
    "            \"Please review it carefully and provide your analysis.\"\n",
    "        )\n",
    "    \n",
    "    async def generate_response(self, question: str, experiment_type: int = 1,\n",
    "                              previous_response: str = None,\n",
    "                              conversation_history: List[Dict] = None) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GPT4Model(LLMBase):\n",
    "    def __init__(self, api_key: str):\n",
    "        super().__init__()\n",
    "        self.client = openai.AsyncOpenAI(api_key=api_key)\n",
    "    \n",
    "    async def generate_response(self, question: str, experiment_type: int = 1,\n",
    "                              previous_response: str = None,\n",
    "                              conversation_history: List[Dict] = None) -> str:\n",
    "        try:\n",
    "            messages = []\n",
    "            if conversation_history:\n",
    "                messages.extend(conversation_history)\n",
    "                \n",
    "            if experiment_type == 1:\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"{self.error_detection_instruction}\\n\\nQuestion: {question}\"\n",
    "                })\n",
    "            elif experiment_type == 2:\n",
    "                content = f\"{self.error_correction_instruction}\\n\\n\"\n",
    "                if previous_response:\n",
    "                    content += f\"Previous response: {previous_response}\\n\\n\"\n",
    "                content += f\"Question: {question}\"\n",
    "                messages.append({\"role\": \"user\", \"content\": content})\n",
    "            else:  # experiment_type == 3\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"{self.context_consistency_instruction}\\n\\nQuestion: {question}\"\n",
    "                })\n",
    "            \n",
    "            response = await self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-2024-08-06\",\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "                max_tokens=1024\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"GPT-4 Error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "class ClaudeModel(LLMBase):\n",
    "    def __init__(self, api_key: str):\n",
    "        super().__init__()\n",
    "        self.client = anthropic.AsyncAnthropic(api_key=api_key)\n",
    "    \n",
    "    async def generate_response(self, question: str, experiment_type: int = 1,\n",
    "                              previous_response: str = None,\n",
    "                              conversation_history: List[Dict] = None) -> str:\n",
    "        try:\n",
    "            messages = []\n",
    "            if conversation_history:\n",
    "                messages.extend(conversation_history)\n",
    "            \n",
    "            if experiment_type == 1:\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"{self.error_detection_instruction}\\n\\nQuestion: {question}\"\n",
    "                })\n",
    "            elif experiment_type == 2:\n",
    "                content = f\"{self.error_correction_instruction}\\n\\n\"\n",
    "                if previous_response:\n",
    "                    content += f\"Previous response: {previous_response}\\n\\n\"\n",
    "                content += f\"Question: {question}\"\n",
    "                messages.append({\"role\": \"user\", \"content\": content})\n",
    "            else:  # experiment_type == 3\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"{self.context_consistency_instruction}\\n\\nQuestion: {question}\"\n",
    "                })\n",
    "            \n",
    "            response = await self.client.messages.create(\n",
    "                model=\"claude-3-5-haiku-20241022\",\n",
    "                max_tokens=1024,\n",
    "                temperature=0,\n",
    "                messages=messages\n",
    "            )\n",
    "            return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f\"Claude Error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "class LlamaModel(LLMBase):\n",
    "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__()\n",
    "        print(f\"Loading Llama model on {device}...\")\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "            device_map=device\n",
    "        )\n",
    "        self.generation_config = GenerationConfig(\n",
    "            max_new_tokens=2048,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    def _create_prompt(self, question: str, experiment_type: int = 1,\n",
    "                      previous_response: str = None,\n",
    "                      conversation_history: List[Dict] = None) -> str:\n",
    "        instruction = self.error_detection_instruction\n",
    "        content = f\"Problem: {question}\"\n",
    "        \n",
    "        if experiment_type == 2:\n",
    "            instruction = self.error_correction_instruction\n",
    "            if previous_response:\n",
    "                content = f\"Previous response: {previous_response}\\n\\nProblem: {question}\"\n",
    "        elif experiment_type == 3 and conversation_history:\n",
    "            instruction = self.context_consistency_instruction\n",
    "            previous_exchanges = []\n",
    "            for msg in conversation_history:\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    previous_exchanges.append(f\"Question: {msg['content']}\")\n",
    "                else:\n",
    "                    previous_exchanges.append(f\"Previous Analysis: {msg['content']}\")\n",
    "            content = \"\\n\\n\".join(previous_exchanges + [f\"Problem: {question}\"])\n",
    "        \n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "        ### Instruction:\n",
    "        {instruction}\n",
    "\n",
    "        {content}\n",
    "\n",
    "        ### Response:\"\"\"\n",
    "    \n",
    "    async def generate_response(self, question: str, experiment_type: int = 1,\n",
    "                              previous_response: str = None,\n",
    "                              conversation_history: List[Dict] = None) -> str:\n",
    "        try:\n",
    "            prompt = self._create_prompt(question, experiment_type, previous_response, conversation_history)\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    generation_config=self.generation_config\n",
    "                )\n",
    "            \n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = response.split(\"### Response:\")[-1].strip()\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Llama Error: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "class QwenModel(LLMBase):\n",
    "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__()\n",
    "        print(f\"Loading Qwen model on {device}...\")\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "            device_map=device\n",
    "        )\n",
    "        self.generation_config = GenerationConfig(\n",
    "            max_new_tokens=2048,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    def _create_prompt(self, question: str, experiment_type: int = 1,\n",
    "                      previous_response: str = None,\n",
    "                      conversation_history: List[Dict] = None) -> str:\n",
    "        instruction = self.error_detection_instruction\n",
    "        content = f\"Question: {question}\"\n",
    "        \n",
    "        if experiment_type == 2:\n",
    "            instruction = self.error_correction_instruction\n",
    "            if previous_response:\n",
    "                content = f\"Previous response: {previous_response}\\n\\nQuestion: {question}\"\n",
    "        elif experiment_type == 3 and conversation_history:\n",
    "            instruction = self.context_consistency_instruction\n",
    "            chat_history = []\n",
    "            for msg in conversation_history:\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    chat_history.append(f\"Previous Question: {msg['content']}\")\n",
    "                else:\n",
    "                    chat_history.append(f\"Your Analysis: {msg['content']}\")\n",
    "            content = \"\\n\\n\".join(chat_history + [f\"Current Question: {question}\"])\n",
    "        \n",
    "        return f\"{instruction}\\n\\n{content}\\nAnswer: \"\n",
    "    \n",
    "    async def generate_response(self, question: str, experiment_type: int = 1,\n",
    "                              previous_response: str = None,\n",
    "                              conversation_history: List[Dict] = None) -> str:\n",
    "        try:\n",
    "            prompt = self._create_prompt(question, experiment_type, previous_response, conversation_history)\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    generation_config=self.generation_config\n",
    "                )\n",
    "            \n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = response.split(\"Answer: \")[-1].strip()\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Qwen Error: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "class GemmaModel(LLMBase):\n",
    "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__()\n",
    "        print(f\"Loading gemma-2 model on {device}...\")\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", \n",
    "                                                      trust_remote_code=True)\n",
    "                                                       \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"google/gemma-2-2b-it\",\n",
    "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "            device_map=device,\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=\"eager\"\n",
    "        )\n",
    "        self.generation_config = GenerationConfig(\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    def _create_prompt(self, question: str, experiment_type: int = 1,\n",
    "                      previous_response: str = None,\n",
    "                      conversation_history: List[Dict] = None) -> str:\n",
    "        instruction = self.error_detection_instruction\n",
    "        content = f\"Question: {question}\"\n",
    "        \n",
    "        if experiment_type == 2:\n",
    "            instruction = self.error_correction_instruction\n",
    "            if previous_response:\n",
    "                content = f\"Previous response: {previous_response}\\n\\nQuestion: {question}\"\n",
    "        elif experiment_type == 3 and conversation_history:\n",
    "            instruction = self.context_consistency_instruction\n",
    "            chat_history = []\n",
    "            for msg in conversation_history:\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    chat_history.append(f\"Previous Question: {msg['content']}\")\n",
    "                else:\n",
    "                    chat_history.append(f\"Your Analysis: {msg['content']}\")\n",
    "            content = \"\\n\\n\".join(chat_history + [f\"Current Question: {question}\"])\n",
    "        \n",
    "        return f\"{instruction}\\n\\n{content}\\nAnswer: \"\n",
    "    \n",
    "    async def generate_response(self, question: str, experiment_type: int = 1,\n",
    "                              previous_response: str = None,\n",
    "                              conversation_history: List[Dict] = None) -> str:\n",
    "        try:\n",
    "            prompt = self._create_prompt(question, experiment_type, previous_response, conversation_history)\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    generation_config=self.generation_config,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            \n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            # print(\"========FULL RES=======\")\n",
    "            # print(response)\n",
    "            response = response.split(\"Answer: \")[-1].strip()\n",
    "            # print(\"========FINAL RES=======\")\n",
    "            # print(response)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Phi Error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "class ResponseAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        self.error_patterns = [                   \n",
    "            \"impossible\", \"contradiction\", \"invalid\", \"cannot exist\",\n",
    "            \"error\", \"incorrect\", \"not possible\", \"inconsistent\",\n",
    "            \"violates\", \"paradox\", \"undefined\", \"ambiguous\",\n",
    "            \"nonsensical\", \"illogical\", \"meaningless\", \"absurd\",\n",
    "            \"problematic\", \"incompatible\", \"unrealistic\", \"infeasible\",\n",
    "            \"mathematically impossible\", \"physically impossible\",\n",
    "            \"defies laws\", \"breaks conservation\", \"not well-defined\",\n",
    "            \"self-contradicting\", \"logically flawed\", \"does not make sense\",\n",
    "            \"cannot be determined\", \"no solution exists\", \"contradicts itself\",\n",
    "            \"violates principle\", \"exceeds bounds\", \"mathematically unsound\",\n",
    "            \"not valid\", \"cannot be true\", \"fails to satisfy\", \"outside realm\", \"discrepancy\"\n",
    "        ]\n",
    "    \n",
    "    def analyze_response(self, response: str, actual_reason: str, \n",
    "                        experiment_type: int = 1, \n",
    "                        previous_response: str = None,\n",
    "                        first_response: str = None) -> Dict[str, Any]:\n",
    "        response_lower = response.lower()\n",
    "        error_detected = any(pattern in response_lower for pattern in self.error_patterns)\n",
    "        \n",
    "        analysis = {\n",
    "            \"error_detected\": error_detected,\n",
    "            \"error_identification_accuracy\": self._calculate_error_identification_accuracy(\n",
    "                response, actual_reason)\n",
    "        }\n",
    "        \n",
    "        if experiment_type == 2 and previous_response:\n",
    "            analysis.update({\n",
    "                \"error_admitted\": error_detected,\n",
    "                \"correction_quality\": self._calculate_correction_quality(\n",
    "                    response, previous_response, actual_reason)\n",
    "            })\n",
    "        \n",
    "        if experiment_type == 3 and first_response:\n",
    "            analysis[\"context_consistency\"] = self._calculate_context_consistency(\n",
    "                response, first_response)\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _calculate_error_identification_accuracy(self, response: str, actual_reason: str) -> float:\n",
    "        embeddings = self.embedding_model.encode([response, actual_reason])\n",
    "        return float(util.cos_sim(embeddings[0], embeddings[1])[0][0])\n",
    "    \n",
    "    def _calculate_correction_quality(self, current_response: str, \n",
    "                                    previous_response: str, actual_reason: str) -> float:\n",
    "        prev_accuracy = self._calculate_error_identification_accuracy(previous_response, actual_reason)\n",
    "        curr_accuracy = self._calculate_error_identification_accuracy(current_response, actual_reason)\n",
    "        return max(0, (curr_accuracy - prev_accuracy) / prev_accuracy)\n",
    "    \n",
    "    def _calculate_context_consistency(self, current_response: str, first_response: str) -> float:\n",
    "        embeddings = self.embedding_model.encode([current_response, first_response])\n",
    "        return float(util.cos_sim(embeddings[0], embeddings[1])[0][0])\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, models: Dict[str, LLMBase], analyzer: ResponseAnalyzer):\n",
    "        self.models = models\n",
    "        self.analyzer = analyzer\n",
    "        self.results = []\n",
    "        self.start_time = None\n",
    "        self.conversation_history = {}\n",
    "    \n",
    "    def _get_conversation_key(self, model_name: str, question: str) -> str:\n",
    "        return f\"{model_name}_{question[:50]}\"\n",
    "    \n",
    "    def _update_conversation_history(self, model_name: str, question: str, \n",
    "                                   role: str, content: str):\n",
    "        key = self._get_conversation_key(model_name, question)\n",
    "        if key not in self.conversation_history:\n",
    "            self.conversation_history[key] = []\n",
    "        self.conversation_history[key].append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    async def run_all_experiments(self, questions: List[Question]):\n",
    "\n",
    "        print(\"\\nStarting Experiment 1: Error Detection...\")\n",
    "        exp1_results = await self.run_experiment(questions, experiment_type=1)\n",
    "        \n",
    "        print(\"\\nStarting Experiment 2: Error Correction...\")\n",
    "        await self.run_experiment(questions, experiment_type=2, \n",
    "                                previous_responses=exp1_results)\n",
    "        \n",
    "        print(\"\\nStarting Experiment 3: Context Consistency...\")\n",
    "        await self.run_experiment(questions, experiment_type=3, \n",
    "                                first_responses=exp1_results)\n",
    "    \n",
    "    async def run_experiment(self, questions: List[Question], experiment_type: int = 1,\n",
    "                           previous_responses: Dict = None,\n",
    "                           first_responses: Dict = None) -> Dict:\n",
    "        self.start_time = datetime.now()\n",
    "        exp_results = {}\n",
    "        \n",
    "        total_questions = len(questions)\n",
    "        total_models = len(self.models)\n",
    "        total_tasks = total_questions * total_models\n",
    "        completed_tasks = 0\n",
    "\n",
    "        print(f\"\\nTotal questions: {total_questions}\")\n",
    "        print(f\"Total models: {total_models}\")\n",
    "        print(f\"Total tasks to process: {total_tasks}\\n\")\n",
    "    \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            for model_name, model in self.models.items():\n",
    "                try:\n",
    "                    conversation_history = None\n",
    "                    if experiment_type == 3:\n",
    "                        key = self._get_conversation_key(model_name, question.content)\n",
    "                        conversation_history = self.conversation_history.get(key, [])\n",
    "                    \n",
    "                    previous_response = None\n",
    "                    if experiment_type == 2 and previous_responses:\n",
    "                        previous_response = previous_responses.get(\n",
    "                            (model_name, question.content))\n",
    "                    \n",
    "                    response = await model.generate_response(\n",
    "                        question.content,\n",
    "                        experiment_type=experiment_type,\n",
    "                        previous_response=previous_response,\n",
    "                        conversation_history=conversation_history\n",
    "                    )\n",
    "                    \n",
    "                    if experiment_type == 3:\n",
    "                        self._update_conversation_history(\n",
    "                            model_name, question.content,\n",
    "                            \"assistant\", response\n",
    "                        )\n",
    "\n",
    "                    first_response = first_responses.get((model_name, question.content)) if first_responses else None\n",
    "                    analysis = self.analyzer.analyze_response(\n",
    "                        response,\n",
    "                        question.fault_reason,\n",
    "                        experiment_type=experiment_type,\n",
    "                        previous_response=previous_response,\n",
    "                        first_response=first_response\n",
    "                    )\n",
    "\n",
    "                    result = {\n",
    "                        \"Experiment\": experiment_type,\n",
    "                        \"Model\": model_name,\n",
    "                        \"Question\": question.content,\n",
    "                        \"Response\": response,\n",
    "                        \"Error Detected\": analysis[\"error_detected\"],\n",
    "                        \"Error Identification Accuracy\": analysis[\"error_identification_accuracy\"]\n",
    "                    }\n",
    "\n",
    "                    if experiment_type == 2:\n",
    "                        result.update({\n",
    "                            \"Error Admitted\": analysis[\"error_admitted\"],\n",
    "                            \"Correction Quality\": analysis[\"correction_quality\"]\n",
    "                        })\n",
    "\n",
    "                    if experiment_type == 3:\n",
    "                        result[\"Context Consistency\"] = analysis.get(\"context_consistency\", 0.0)\n",
    "\n",
    "                    self.results.append(result)\n",
    "\n",
    "                    if experiment_type == 1:\n",
    "                        exp_results[(model_name, question.content)] = response\n",
    "                        \n",
    "                    completed_tasks += 1\n",
    "                    if completed_tasks % 10 == 0:  \n",
    "                        print(f\"Completed {completed_tasks}/{total_tasks} tasks ({(completed_tasks/total_tasks)*100:.1f}%)\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError in Experiment {experiment_type} with {model_name}: {str(e)}\")\n",
    "\n",
    "                await asyncio.sleep(1)\n",
    "\n",
    "        return exp_results if experiment_type == 1 else None\n",
    "\n",
    "    def generate_report(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        df = pd.DataFrame(self.results)\n",
    "\n",
    "        metrics = []\n",
    "        for model_name in self.models.keys():\n",
    "            for experiment in [1, 2, 3]:\n",
    "                model_exp_data = df[\n",
    "                    (df[\"Model\"] == model_name) & \n",
    "                    (df[\"Experiment\"] == experiment)\n",
    "                ]\n",
    "\n",
    "                metric = {\n",
    "                    \"Model\": model_name,\n",
    "                    \"Experiment\": experiment,\n",
    "                    \"Error Detection Rate\": model_exp_data[\"Error Detected\"].mean(),\n",
    "                    \"Error Identification Accuracy\": model_exp_data[\"Error Identification Accuracy\"].mean()\n",
    "                }\n",
    "\n",
    "                if experiment == 2:\n",
    "\n",
    "                    exp1_missed_errors = df[\n",
    "                        (df[\"Model\"] == model_name) & \n",
    "                        (df[\"Experiment\"] == 1) & \n",
    "                        (df[\"Error Detected\"] == False)\n",
    "                    ][\"Question\"].tolist()\n",
    "\n",
    "                    filtered_exp2_data = model_exp_data[\n",
    "                        model_exp_data[\"Question\"].isin(exp1_missed_errors)\n",
    "                    ]\n",
    "\n",
    "                    if len(filtered_exp2_data) > 0:\n",
    "                        metric.update({\n",
    "                            \"Error Admission Rate\": filtered_exp2_data[\"Error Admitted\"].mean(),\n",
    "                            \"Correction Quality\": filtered_exp2_data[\"Correction Quality\"].mean(),\n",
    "                            \"Questions Analyzed\": len(filtered_exp2_data)\n",
    "                        })\n",
    "                    else:\n",
    "                        metric.update({\n",
    "                            \"Error Admission Rate\": 0.0,\n",
    "                            \"Correction Quality\": 0.0,\n",
    "                            \"Questions Analyzed\": 0\n",
    "                        })\n",
    "\n",
    "                if experiment == 3:\n",
    "                    metric[\"Context Consistency Rate\"] = model_exp_data[\"Context Consistency\"].mean()\n",
    "\n",
    "                metrics.append(metric)\n",
    "\n",
    "        metrics_df = pd.DataFrame(metrics)\n",
    "        return df, metrics_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a11c1b9-f53a-4820-9c33-0df540e550c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # GPU 사용 가능 여부 확인\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # API keys\n",
    "    OPENAI_API_KEY = \"your_openai_key\"\n",
    "    ANTHROPIC_API_KEY = \"your_anthropic_key\"\n",
    "\n",
    "    try:\n",
    "        # Load data from CSV\n",
    "        print(\"Loading questions from CSV...\")\n",
    "        df = pd.read_csv('final_dataset_with_responses_20241129_encoded_600row_shuffled.csv')\n",
    "        questions = [\n",
    "            Question(\n",
    "                discipline=row['Discipline'],\n",
    "                content=row['Question'],\n",
    "                fault_reason=row['Reason you think it is faulty']\n",
    "            ) for _, row in df.iterrows()\n",
    "        ]\n",
    "        print(f\"Loaded {len(questions)} questions\")\n",
    "\n",
    "        # Optional: Limit questions for testing\n",
    "        # questions = questions[:3]  \n",
    "        print(\"\\nInitializing models...\")\n",
    "        models = {\n",
    "            \"GPT-4\": GPT4Model(OPENAI_API_KEY),\n",
    "            \"Claude-3\": ClaudeModel(ANTHROPIC_API_KEY),\n",
    "            \"Llama-3.2\": LlamaModel(device=device),\n",
    "            \"Qwen-2.5\": QwenModel(device=device)\n",
    "            \"Phi-3.5\": PhiModel(device=device) \n",
    "        }\n",
    "\n",
    "        print(\"\\nInitializing analyzer...\")\n",
    "        analyzer = ResponseAnalyzer()\n",
    "        runner = ExperimentRunner(models, analyzer)\n",
    "\n",
    "        print(\"\\nStarting experiments...\")\n",
    "        total_start_time = datetime.now()\n",
    "        await runner.run_all_experiments(questions)\n",
    "        total_end_time = datetime.now()\n",
    "\n",
    "        print(f\"\\nAll experiments completed in {total_end_time - total_start_time}\")\n",
    "\n",
    "        # Generate and save reports\n",
    "        print(\"\\nGenerating reports...\")\n",
    "        results_df, metrics_df = runner.generate_report()\n",
    "\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results_path = f\"results_{timestamp}.csv\"\n",
    "        metrics_path = f\"metrics_{timestamp}.csv\"\n",
    "\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "        print(f\"\\nResults saved to {results_path}\")\n",
    "        print(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "        print(\"\\nMetrics Summary by Experiment:\")\n",
    "        for experiment in [1, 2, 3]:\n",
    "            print(f\"\\nExperiment {experiment} Results:\")\n",
    "            exp_metrics = metrics_df[metrics_df[\"Experiment\"] == experiment]\n",
    "            print(exp_metrics.to_string())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in main: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c2c98a-6da9-4249-a372-847739dde9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading questions from CSV...\n",
      "Loaded 600 questions\n",
      "\n",
      "Initializing models...\n",
      "\n",
      "Initializing analyzer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec1c85be9544a2195a5ac72c3d93dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0109e4be1c432a936f1223efbb4cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f82a7b982064e06a14075fe6cf1e948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd0a9ec96a8471e9f5db4dfe19caf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b6fd003af74ee69cb0ea3cb8e1db0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcd33468aac404987b6bdc2e4d7e172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d04bb59f74c4564972bc408ee268267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c374f1f97d24d6cb1e2768c5a8b87bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb2b7b8c8c247c7959cf4d2706dca98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d594b8b87d494680e38afd63b94cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93f06aff3ef4841aaba9400b05fe573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting experiments...\n",
      "\n",
      "Starting Experiment 1: Error Detection...\n",
      "\n",
      "Total questions: 600\n",
      "Total models: 1\n",
      "Total tasks to process: 600\n",
      "\n",
      "Completed 10/600 tasks (1.7%)\n",
      "Completed 20/600 tasks (3.3%)\n",
      "Completed 30/600 tasks (5.0%)\n",
      "Completed 40/600 tasks (6.7%)\n",
      "Completed 50/600 tasks (8.3%)\n",
      "Completed 60/600 tasks (10.0%)\n",
      "Completed 70/600 tasks (11.7%)\n",
      "Completed 80/600 tasks (13.3%)\n",
      "Completed 90/600 tasks (15.0%)\n",
      "Completed 100/600 tasks (16.7%)\n",
      "Completed 110/600 tasks (18.3%)\n",
      "Completed 120/600 tasks (20.0%)\n",
      "Completed 130/600 tasks (21.7%)\n",
      "Completed 550/600 tasks (91.7%)\n",
      "Completed 560/600 tasks (93.3%)\n",
      "Completed 570/600 tasks (95.0%)\n",
      "Completed 580/600 tasks (96.7%)\n",
      "Completed 590/600 tasks (98.3%)\n",
      "Completed 600/600 tasks (100.0%)\n",
      "\n",
      "Starting Experiment 2: Error Correction...\n",
      "\n",
      "Total questions: 600\n",
      "Total models: 1\n",
      "Total tasks to process: 600\n",
      "\n",
      "Completed 10/600 tasks (1.7%)\n",
      "Completed 160/600 tasks (26.7%)\n",
      "Completed 170/600 tasks (28.3%)\n",
      "Completed 180/600 tasks (30.0%)\n",
      "Completed 190/600 tasks (31.7%)\n",
      "Completed 200/600 tasks (33.3%)\n",
      "Completed 210/600 tasks (35.0%)\n",
      "Completed 220/600 tasks (36.7%)\n",
      "Completed 230/600 tasks (38.3%)\n",
      "Completed 240/600 tasks (40.0%)\n",
      "Completed 250/600 tasks (41.7%)\n",
      "Completed 260/600 tasks (43.3%)\n",
      "Completed 270/600 tasks (45.0%)\n",
      "Completed 280/600 tasks (46.7%)\n",
      "Completed 290/600 tasks (48.3%)\n",
      "Completed 300/600 tasks (50.0%)\n",
      "Completed 310/600 tasks (51.7%)\n",
      "Completed 320/600 tasks (53.3%)\n",
      "Completed 330/600 tasks (55.0%)\n",
      "Completed 340/600 tasks (56.7%)\n",
      "Completed 350/600 tasks (58.3%)\n",
      "Completed 360/600 tasks (60.0%)\n",
      "Completed 370/600 tasks (61.7%)\n",
      "Completed 380/600 tasks (63.3%)\n",
      "Completed 390/600 tasks (65.0%)\n",
      "Completed 400/600 tasks (66.7%)\n",
      "Completed 410/600 tasks (68.3%)\n",
      "Completed 420/600 tasks (70.0%)\n",
      "Completed 430/600 tasks (71.7%)\n",
      "Completed 440/600 tasks (73.3%)\n",
      "Completed 450/600 tasks (75.0%)\n",
      "Completed 460/600 tasks (76.7%)\n",
      "Completed 470/600 tasks (78.3%)\n",
      "Completed 480/600 tasks (80.0%)\n",
      "Completed 490/600 tasks (81.7%)\n",
      "Completed 500/600 tasks (83.3%)\n",
      "Completed 510/600 tasks (85.0%)\n",
      "Completed 520/600 tasks (86.7%)\n",
      "Completed 530/600 tasks (88.3%)\n",
      "Completed 540/600 tasks (90.0%)\n",
      "Completed 550/600 tasks (91.7%)\n",
      "Completed 560/600 tasks (93.3%)\n",
      "Completed 570/600 tasks (95.0%)\n",
      "Completed 580/600 tasks (96.7%)\n",
      "Completed 590/600 tasks (98.3%)\n",
      "Completed 600/600 tasks (100.0%)\n",
      "\n",
      "Starting Experiment 3: Context Consistency...\n",
      "\n",
      "Total questions: 600\n",
      "Total models: 1\n",
      "Total tasks to process: 600\n",
      "\n",
      "Completed 10/600 tasks (1.7%)\n",
      "Completed 20/600 tasks (3.3%)\n",
      "Completed 30/600 tasks (5.0%)\n",
      "Completed 40/600 tasks (6.7%)\n",
      "Completed 50/600 tasks (8.3%)\n",
      "Completed 60/600 tasks (10.0%)\n",
      "Completed 70/600 tasks (11.7%)\n",
      "Completed 80/600 tasks (13.3%)\n",
      "Completed 90/600 tasks (15.0%)\n",
      "Completed 100/600 tasks (16.7%)\n",
      "Completed 110/600 tasks (18.3%)\n",
      "Completed 120/600 tasks (20.0%)\n",
      "Completed 130/600 tasks (21.7%)\n",
      "Completed 140/600 tasks (23.3%)\n",
      "Completed 150/600 tasks (25.0%)\n",
      "Completed 160/600 tasks (26.7%)\n",
      "Completed 170/600 tasks (28.3%)\n",
      "Completed 180/600 tasks (30.0%)\n",
      "Completed 190/600 tasks (31.7%)\n",
      "Completed 200/600 tasks (33.3%)\n",
      "Completed 210/600 tasks (35.0%)\n",
      "Completed 220/600 tasks (36.7%)\n",
      "Completed 230/600 tasks (38.3%)\n",
      "Completed 240/600 tasks (40.0%)\n",
      "Completed 250/600 tasks (41.7%)\n",
      "Completed 260/600 tasks (43.3%)\n",
      "Completed 270/600 tasks (45.0%)\n",
      "Completed 280/600 tasks (46.7%)\n",
      "Completed 290/600 tasks (48.3%)\n",
      "Completed 310/600 tasks (51.7%)\n",
      "Completed 320/600 tasks (53.3%)\n",
      "Completed 330/600 tasks (55.0%)\n",
      "Completed 340/600 tasks (56.7%)\n",
      "Completed 350/600 tasks (58.3%)\n",
      "Completed 360/600 tasks (60.0%)\n",
      "Completed 370/600 tasks (61.7%)\n",
      "Completed 380/600 tasks (63.3%)\n",
      "Completed 390/600 tasks (65.0%)\n",
      "Completed 400/600 tasks (66.7%)\n",
      "Completed 410/600 tasks (68.3%)\n",
      "Completed 420/600 tasks (70.0%)\n",
      "Completed 430/600 tasks (71.7%)\n",
      "Completed 440/600 tasks (73.3%)\n",
      "Completed 450/600 tasks (75.0%)\n",
      "Completed 460/600 tasks (76.7%)\n",
      "Completed 470/600 tasks (78.3%)\n",
      "Completed 480/600 tasks (80.0%)\n",
      "Completed 490/600 tasks (81.7%)\n",
      "Completed 500/600 tasks (83.3%)\n",
      "Completed 510/600 tasks (85.0%)\n",
      "Completed 520/600 tasks (86.7%)\n",
      "Completed 530/600 tasks (88.3%)\n",
      "Completed 540/600 tasks (90.0%)\n",
      "Completed 550/600 tasks (91.7%)\n",
      "Completed 560/600 tasks (93.3%)\n",
      "Completed 570/600 tasks (95.0%)\n",
      "Completed 580/600 tasks (96.7%)\n",
      "Completed 590/600 tasks (98.3%)\n",
      "Completed 600/600 tasks (100.0%)\n",
      "\n",
      "All experiments completed in 3:06:09.501581\n",
      "\n",
      "Generating reports...\n",
      "\n",
      "Results saved to results_20241201_221520.csv\n",
      "Metrics saved to metrics_20241201_221520.csv\n",
      "\n",
      "Metrics Summary by Experiment:\n",
      "\n",
      "Experiment 1 Results:\n",
      "      Model  Experiment  Error Detection Rate  Error Identification Accuracy  Error Admission Rate  Correction Quality  Questions Analyzed  Context Consistency Rate\n",
      "0  Claude-3           1              0.083333                       0.537889                   NaN                 NaN                 NaN                       NaN\n",
      "\n",
      "Experiment 2 Results:\n",
      "      Model  Experiment  Error Detection Rate  Error Identification Accuracy  Error Admission Rate  Correction Quality  Questions Analyzed  Context Consistency Rate\n",
      "1  Claude-3           2                 0.335                       0.569324              0.292727            0.098702               550.0                       NaN\n",
      "\n",
      "Experiment 3 Results:\n",
      "      Model  Experiment  Error Detection Rate  Error Identification Accuracy  Error Admission Rate  Correction Quality  Questions Analyzed  Context Consistency Rate\n",
      "2  Claude-3           3                 0.135                       0.547102                   NaN                 NaN                 NaN                   0.92633\n"
     ]
    }
   ],
   "source": [
    "await main() #only Claude 3.5 Model executed Result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
